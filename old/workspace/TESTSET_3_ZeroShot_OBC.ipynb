{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da2286-887b-4791-bfb7-4d55e3ffe0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET (reloading independent code fragment for now sampled excel)\n",
    "\n",
    "# Import CNN Zero-Shot & other necessary stuff\n",
    "from transformers import pipeline  # It takes time here\n",
    "\n",
    "# For data input and data cleaning\n",
    "import pandas as pd\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import NaN\n",
    "import time\n",
    "\n",
    "# Available directories (input yours for personal use)\n",
    "cep_data_dir = \"/home/aan0709@tmme/pcu-research-mapping/data/\"\n",
    "\n",
    "working_dir = \"TEST/\"  # CHANGE WORKING DIRECTORY INSIDE IEEE XPLORE FOLDER\n",
    "\n",
    "\n",
    "# For data importing\n",
    "def openExcel(excel_name, sheet_name, directory):\n",
    "    df = pd.read_excel(directory + excel_name + \".xlsx\", sheet_name=sheet_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleanIndexes(df):\n",
    "    temp_ls = df.columns\n",
    "    for i in df.columns:\n",
    "        if i == \"abstract\":\n",
    "            break\n",
    "        df.drop(columns=[i], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Select INPUT EXCEL\n",
    "file_name = \"TESTSET_Charger_answered\"  # -> I have edited the Affiliations and the Keywords part\n",
    "\n",
    "df = openExcel(file_name, \"Sheet1\", cep_data_dir + working_dir)\n",
    "\n",
    "df = cleanIndexes(df)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75bfdd42-59e5-4acb-b29a-bac2109430fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds elapsed:  11.05048656463623\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "classifierGPU0 = pipeline(\n",
    "    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=1\n",
    ")\n",
    "# classifierGPU0 = pipeline(\"zero-shot-classification\", model='roberta-large-mnli', device=-1)\n",
    "# classifierGPU = pipeline(\"zero-shot-classification\", model='xlm-roberta-large-finetuned-conll03-english', device=-1)\n",
    "# classifierGPU = pipeline(\"zero-shot-classification\", model='finiteautomata/beto-sentiment-analysis', device=-1)\n",
    "# classifierGPU = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-distilroberta-base', device=-1)\n",
    "\n",
    "end = time.time()\n",
    "print(\"seconds elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a30330-7980-479c-b821-66f06c46403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 0\n",
    "\n",
    "positive_outcome = \"onboard charger on-board charger\"\n",
    "# negative_outcome = \"not traction inverter\"\n",
    "\n",
    "candidate_labels = [positive_outcome]  # , negative_outcome]\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e2b179-3385-48c3-aa11-ae26cba6c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1\n",
    "\n",
    "positive_outcome = \"onboard charger on-board charger motor windings vehicle\"\n",
    "negative_outcome = \"control grid\"\n",
    "\n",
    "candidate_labels = [positive_outcome, negative_outcome]\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7838c9eb-54c0-43d4-9e59-2ee8aceeb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2\n",
    "\n",
    "positive_outcome = \"onboard charger on-board charger integrated charger  motor  vehicle double active bridge DAB resonant converter LLC\"\n",
    "\n",
    "candidate_labels = [\n",
    "    \"onboard charger\",\n",
    "    \"on-board charger\",\n",
    "    \"integrated charger\",\n",
    "    \"motor\",\n",
    "    \"vehicle\",\n",
    "    \"double active bridge DAB\",\n",
    "    \"resonant converter LLC\",\n",
    "]  # \"control\", \"grid\", \"inverter\", \"wireless\"]\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1a23804-995a-4041-a64b-b34169a62107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aan0709@tmme/.local/lib/python3.7/site-packages/transformers/pipelines/base.py:978: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Time: 0 min 1.657 sec\n",
      "From 50 papers, the 34 are CORRECT and the 16 are WRONG ( 68.00 percent correct )\n",
      "\n",
      "0.05\n",
      "Time: 0 min 1.626 sec\n",
      "From 50 papers, the 41 are CORRECT and the 9 are WRONG ( 82.00 percent correct )\n",
      "\n",
      "0.1\n",
      "Time: 0 min 1.638 sec\n",
      "From 50 papers, the 43 are CORRECT and the 7 are WRONG ( 86.00 percent correct )\n",
      "\n",
      "0.15\n",
      "Time: 0 min 1.636 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "0.2\n",
      "Time: 0 min 1.655 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "0.25\n",
      "Time: 0 min 1.656 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "0.3\n",
      "Time: 0 min 1.628 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "0.35\n",
      "Time: 0 min 1.669 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "0.4\n",
      "Time: 0 min 1.675 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "0.45\n",
      "Time: 0 min 1.653 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "0.5\n",
      "Time: 0 min 1.679 sec\n",
      "From 50 papers, the 45 are CORRECT and the 5 are WRONG ( 90.00 percent correct )\n",
      "\n",
      "0.55\n",
      "Time: 0 min 1.678 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "0.6\n",
      "Time: 0 min 1.628 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "0.65\n",
      "Time: 0 min 1.641 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "0.7\n",
      "Time: 0 min 1.645 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "0.75\n",
      "Time: 0 min 1.681 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "0.8\n",
      "Time: 0 min 1.678 sec\n",
      "From 50 papers, the 43 are CORRECT and the 7 are WRONG ( 86.00 percent correct )\n",
      "\n",
      "0.85\n",
      "Time: 0 min 1.705 sec\n",
      "From 50 papers, the 43 are CORRECT and the 7 are WRONG ( 86.00 percent correct )\n",
      "\n",
      "0.9\n",
      "Time: 0 min 1.693 sec\n",
      "From 50 papers, the 36 are CORRECT and the 14 are WRONG ( 72.00 percent correct )\n",
      "\n",
      "0.95\n",
      "Time: 0 min 1.669 sec\n",
      "From 50 papers, the 30 are CORRECT and the 20 are WRONG ( 60.00 percent correct )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "percentages = []\n",
    "for perc_loop in range(0, 100, 5):\n",
    "\n",
    "    en_stopwords = []\n",
    "    en_stopwords = list(get_stop_words(\"en\"))  # About 900 stopwords\n",
    "    nltk_words = list(stopwords.words(\"english\"))  # About 150 stopwords\n",
    "    en_stopwords.extend(nltk_words)\n",
    "\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    Author_Keywords = \"index_terms.author_terms.terms\"\n",
    "    IEEE_Keywords = \"index_terms.ieee_terms.terms\"\n",
    "\n",
    "    zero_shot_judgement = []\n",
    "    # df[\"ZeroShot Judgement\"] = NaN\n",
    "\n",
    "    count_all = len(df)\n",
    "    count_diff = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    final_judgement_dict = {}\n",
    "    for i in range(len(df)):\n",
    "        # for i in range(3):\n",
    "        data = \"\"\n",
    "\n",
    "        # Where to search\n",
    "        if isinstance(df.loc[i, \"abstract\"], str):\n",
    "            data += df.loc[i, \"abstract\"]\n",
    "        if isinstance(df.loc[i, \"title\"], str):\n",
    "            data += \" \" + df.loc[i, \"title\"]\n",
    "        exception_to_str = df.loc[i, IEEE_Keywords]\n",
    "        if isinstance(exception_to_str, str):\n",
    "            exception_to_str = \" \".join(df.loc[i, IEEE_Keywords].split(\";\"))\n",
    "            data += \" \" + exception_to_str\n",
    "        exception_to_str = df.loc[i, Author_Keywords]\n",
    "        if isinstance(exception_to_str, str):\n",
    "            exception_to_str = \" \".join(df.loc[i, Author_Keywords].split(\";\"))\n",
    "            data += \" \" + exception_to_str\n",
    "            printing_data = data\n",
    "\n",
    "        # Data Cleaning with NLTK, NumPy\n",
    "        tokens = regexp_tokenize(data, pattern=r\"\\s|[\\.,;'()]\", gaps=True)\n",
    "        words = []\n",
    "        for k in tokens:\n",
    "            if k not in en_stopwords:\n",
    "                k = lemma.lemmatize(k)\n",
    "            words.append(k)\n",
    "        data = \" \".join(words)\n",
    "\n",
    "        # Here the data has been cleaned\n",
    "        input_text = data\n",
    "\n",
    "        # Rienforcing the data with keyword\n",
    "        tokens = regexp_tokenize(data, pattern=r\"\\s|[\\&<>\\.,;'()]\", gaps=True)\n",
    "        for j in tokens:  # For every word in the data:\n",
    "            lc_j = j.lower()\n",
    "            if (\n",
    "                len(j) > 3\n",
    "            ):  # this excludes ie. the BJTs keyword, so be careful (but also filters out words like \"and\" etc)\n",
    "                for k in candidate_labels:\n",
    "                    lc_k = k.lower()\n",
    "                    if lc_j in lc_k:\n",
    "                        data += \" \" + j  # + \" \" + j)\n",
    "\n",
    "        start_judge = time.time()\n",
    "        # Extract the probabilities from the CNN\n",
    "        final_judgement_dict = classifierGPU0(\n",
    "            input_text,\n",
    "            candidate_labels,\n",
    "            hypothesis_template=hypothesis_template,\n",
    "            multi_label=False,\n",
    "        )\n",
    "\n",
    "        end_judge = time.time()\n",
    "\n",
    "        if (\n",
    "            final_judgement_dict[\"labels\"][0] in positive_outcome\n",
    "            and final_judgement_dict[\"scores\"][0] > perc_loop / 100.0\n",
    "        ):\n",
    "            zero_shot_judge = 1\n",
    "        else:\n",
    "            zero_shot_judge = 0\n",
    "        # print(zero_shot_judge)\n",
    "        zero_shot_judgement.append(zero_shot_judge)\n",
    "        df.loc[i, \"ZeroShot Judgement\"] = float(zero_shot_judge)\n",
    "\n",
    "        # print(data)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Print differences\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, \"ZeroShot Judgement\"] != df.loc[i, \"Manual Judgement\"]:\n",
    "            count_diff += 1\n",
    "\n",
    "            \"\"\"print(df.loc[i, \"Manual Judgement\"])\n",
    "            print(\"----- DECISION ----- :\")\n",
    "            if str(zero_shot_judgement[i]) == \"1\":\n",
    "                print(\"-- It is included --\")\n",
    "            else:\n",
    "                print(\"-- It is NNNOOOTTT included --\")    # Visual exageration to easily spot answer\n",
    "            print()\n",
    "            if isinstance(df.loc[i, Author_Keywords], str): auth_keys = df.loc[i, Author_Keywords].split(\";\")\n",
    "            else: auth_keys = \"\"\n",
    "            if isinstance(df.loc[i, IEEE_Keywords], str): ieee_keys = df.loc[i, IEEE_Keywords].split(\";\")\n",
    "            else: ieee_keys = \"\"\n",
    "            print(\"Abstract :\")\n",
    "            print(df.loc[i, \"abstract\"], \"\\n\")\n",
    "            print(\"Document Title :\")\n",
    "            print(df.loc[i, \"title\"], \"\\n\")\n",
    "            print(\"Author Keywords :\")\n",
    "            [print(\"-> \", x) for x in auth_keys]\n",
    "            print()\n",
    "            print(\"IEEE Keywords :\")\n",
    "            [print(\"-> \", x) for x in ieee_keys]\"\"\"\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, \"ZeroShot Judgement\"] == \"1\":\n",
    "            count += 1\n",
    "\n",
    "    percentages.append((count_all - count_diff) / count_all)\n",
    "    print(perc_loop / 100)\n",
    "    m, s = divmod(end - start, 60)\n",
    "    print(\"Time: %d min %0.3f sec\" % (m, s))\n",
    "    print(\n",
    "        \"From %d papers, the %d are CORRECT and the %d are WRONG ( %.2f percent correct )\\n\"\n",
    "        % (\n",
    "            count_all,\n",
    "            count_all - count_diff,\n",
    "            count_diff,\n",
    "            (100 * (count_all - count_diff) / count_all),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98e8b8-5118-4a67-adf4-49cef424880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[i, \"ZeroShot Judgement\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP (python3.7)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
