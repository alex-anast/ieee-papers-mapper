{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da2286-887b-4791-bfb7-4d55e3ffe0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET (reloading independent code fragment for now sampled excel)\n",
    "\n",
    "# Import CNN Zero-Shot & other necessary stuff\n",
    "from transformers import pipeline  # It takes time here\n",
    "\n",
    "# For data input and data cleaning\n",
    "import pandas as pd\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import NaN\n",
    "import time\n",
    "\n",
    "# Available directories (input yours for personal use)\n",
    "cep_data_dir = \"/home/aan0709@tmme/pcu-research-mapping/data/\"\n",
    "\n",
    "working_dir = \"TEST/\"  # CHANGE WORKING DIRECTORY INSIDE IEEE XPLORE FOLDER\n",
    "\n",
    "\n",
    "# For data importing\n",
    "def openExcel(excel_name, sheet_name, directory):\n",
    "    df = pd.read_excel(directory + excel_name + \".xlsx\", sheet_name=sheet_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleanIndexes(df):\n",
    "    temp_ls = df.columns\n",
    "    for i in df.columns:\n",
    "        if i == \"abstract\":\n",
    "            break\n",
    "        df.drop(columns=[i], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Select INPUT EXCEL\n",
    "file_name = (\n",
    "    \"TESTSET_GaN_answered\"  # -> I have edited the Affiliations and the Keywords part\n",
    ")\n",
    "\n",
    "df = openExcel(file_name, \"Sheet1\", cep_data_dir + working_dir)\n",
    "\n",
    "df = cleanIndexes(df)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75bfdd42-59e5-4acb-b29a-bac2109430fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds elapsed:  11.147302627563477\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "classifierGPU0 = pipeline(\n",
    "    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=3\n",
    ")\n",
    "end = time.time()\n",
    "print(\"seconds elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d72a80-b048-4249-bee9-56b80300854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 50 papers, the 33 are CORRECT and the 17 are WRONG ( 66.00 percent correct )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_diff = 0\n",
    "count_all = len(df)\n",
    "\n",
    "# Print differences\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i, \"Manual Judgement\"] != df.loc[i, \"ZeroShot Judgement\"]:\n",
    "        count_diff += 1\n",
    "\n",
    "print(\n",
    "    \"From %d papers, the %d are CORRECT and the %d are WRONG ( %.2f percent correct )\\n\"\n",
    "    % (\n",
    "        count_all,\n",
    "        count_all - count_diff,\n",
    "        count_diff,\n",
    "        (100 * (count_all - count_diff) / count_all),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dacfe3b6-93b6-4aba-832e-e77ae5c732fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 50 papers, the 17 are CORRECT and the 33 are WRONG ( 34.00 percent correct )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_diff = 0\n",
    "count_all = len(df)\n",
    "\n",
    "# Print differences\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i, \"Manual Judgement\"] == 0:\n",
    "        count_diff += 1\n",
    "\n",
    "print(\n",
    "    \"From %d papers, the %d are CORRECT and the %d are WRONG ( %.2f percent correct )\\n\"\n",
    "    % (\n",
    "        count_all,\n",
    "        count_all - count_diff,\n",
    "        count_diff,\n",
    "        (100 * (count_all - count_diff) / count_all),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79a30330-7980-479c-b821-66f06c46403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 0 - best @ >?? w/ IEEE Keywords\n",
    "\n",
    "positive_outcome = \"gallium nitride\"\n",
    "# negative_outcome = \"not traction inverter\"\n",
    "\n",
    "candidate_labels = [positive_outcome]  # , negative_outcome]\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e2b179-3385-48c3-aa11-ae26cba6c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1 - best @ >0.1 w/ IEEE Keywords - 96% w/ and w/o abstract\n",
    "\n",
    "positive_outcome = \"gallium nitride HEMT\"\n",
    "negative_outcome = (\n",
    "    \"generative adversarial network light emitting diode photodetector optics laser\"\n",
    ")\n",
    "\n",
    "candidate_labels = [positive_outcome, negative_outcome]\n",
    "\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7838c9eb-54c0-43d4-9e59-2ee8aceeb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2 - worst solution\n",
    "\n",
    "# It's not working well. Gallium nitride is repeated as Gallium Gallium nitride nitride which confuses the model.\n",
    "# Also, double repeating doesn't help because the category's IEEE keyword is too strong to be repeated twice.\n",
    "\n",
    "positive_outcome = \"gallium nitride GaN HEMT FET switches wide band gap semiconductors\"\n",
    "negative_outcome = \"generative adversarial network light emitting diode diode photodetector optics laser\"\n",
    "\n",
    "candidate_labels = [\"gallium nitride\", \"GaN\"]\n",
    "\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e60938-8c17-4fbb-8d73-287ce4b49563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 2.89\n",
      "0.0\n",
      "Time: 0 min 2.894 sec\n",
      "From 50 papers, the 43 are CORRECT and the 7 are WRONG ( 86.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.37\n",
      "0.05\n",
      "Time: 0 min 1.368 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.39\n",
      "0.1\n",
      "Time: 0 min 1.393 sec\n",
      "From 50 papers, the 47 are CORRECT and the 3 are WRONG ( 94.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.38\n",
      "0.15\n",
      "Time: 0 min 1.382 sec\n",
      "From 50 papers, the 48 are CORRECT and the 2 are WRONG ( 96.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.36\n",
      "0.2\n",
      "Time: 0 min 1.360 sec\n",
      "From 50 papers, the 48 are CORRECT and the 2 are WRONG ( 96.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.39\n",
      "0.25\n",
      "Time: 0 min 1.387 sec\n",
      "From 50 papers, the 48 are CORRECT and the 2 are WRONG ( 96.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.41\n",
      "0.3\n",
      "Time: 0 min 1.415 sec\n",
      "From 50 papers, the 47 are CORRECT and the 3 are WRONG ( 94.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.41\n",
      "0.35\n",
      "Time: 0 min 1.414 sec\n",
      "From 50 papers, the 48 are CORRECT and the 2 are WRONG ( 96.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.41\n",
      "0.4\n",
      "Time: 0 min 1.406 sec\n",
      "From 50 papers, the 48 are CORRECT and the 2 are WRONG ( 96.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.37\n",
      "0.45\n",
      "Time: 0 min 1.375 sec\n",
      "From 50 papers, the 48 are CORRECT and the 2 are WRONG ( 96.00 percent correct )\n",
      "\n",
      "#papers= 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aan0709@tmme/.local/lib/python3.7/site-packages/transformers/pipelines/base.py:978: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Papers = 50, Time elapsed: 1.37\n",
      "0.5\n",
      "Time: 0 min 1.375 sec\n",
      "From 50 papers, the 48 are CORRECT and the 2 are WRONG ( 96.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.40\n",
      "0.55\n",
      "Time: 0 min 1.397 sec\n",
      "From 50 papers, the 47 are CORRECT and the 3 are WRONG ( 94.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.39\n",
      "0.6\n",
      "Time: 0 min 1.388 sec\n",
      "From 50 papers, the 47 are CORRECT and the 3 are WRONG ( 94.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.40\n",
      "0.65\n",
      "Time: 0 min 1.401 sec\n",
      "From 50 papers, the 47 are CORRECT and the 3 are WRONG ( 94.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.37\n",
      "0.7\n",
      "Time: 0 min 1.368 sec\n",
      "From 50 papers, the 46 are CORRECT and the 4 are WRONG ( 92.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.37\n",
      "0.75\n",
      "Time: 0 min 1.371 sec\n",
      "From 50 papers, the 45 are CORRECT and the 5 are WRONG ( 90.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.38\n",
      "0.8\n",
      "Time: 0 min 1.379 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.35\n",
      "0.85\n",
      "Time: 0 min 1.355 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.37\n",
      "0.9\n",
      "Time: 0 min 1.374 sec\n",
      "From 50 papers, the 42 are CORRECT and the 8 are WRONG ( 84.00 percent correct )\n",
      "\n",
      "#papers= 25\n",
      "#Papers = 50, Time elapsed: 1.38\n",
      "0.95\n",
      "Time: 0 min 1.381 sec\n",
      "From 50 papers, the 37 are CORRECT and the 13 are WRONG ( 74.00 percent correct )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "percentages = []\n",
    "for perc_loop in range(0, 100, 5):\n",
    "\n",
    "    en_stopwords = []\n",
    "    en_stopwords = list(get_stop_words(\"en\"))  # About 900 stopwords\n",
    "    nltk_words = list(stopwords.words(\"english\"))  # About 150 stopwords\n",
    "    en_stopwords.extend(nltk_words)\n",
    "\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    Author_Keywords = \"index_terms.author_terms.terms\"\n",
    "    IEEE_Keywords = \"index_terms.ieee_terms.terms\"\n",
    "\n",
    "    count_all = len(df)\n",
    "    count_diff = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    input_text = []  # This is the list that will be my dataset for the pipeline\n",
    "    input_index = []\n",
    "    final_judgement_dict = {}\n",
    "    for i in range(len(df)):\n",
    "        data = \"\"\n",
    "        # Where to search\n",
    "        if isinstance(df.loc[i, \"abstract\"], str):\n",
    "            data += df.loc[i, \"abstract\"]\n",
    "        if isinstance(df.loc[i, \"title\"], str):\n",
    "            data += \" \" + df.loc[i, \"title\"]\n",
    "        exception_to_str = df.loc[i, IEEE_Keywords]\n",
    "        if isinstance(exception_to_str, str):\n",
    "            exception_to_str = \" \".join(df.loc[i, IEEE_Keywords].split(\";\"))\n",
    "            data += \" \" + exception_to_str\n",
    "        exception_to_str = df.loc[i, Author_Keywords]\n",
    "        if isinstance(exception_to_str, str):\n",
    "            exception_to_str = \" \".join(df.loc[i, Author_Keywords].split(\";\"))\n",
    "            data += \" \" + exception_to_str\n",
    "            # printing_data = data\n",
    "\n",
    "        # Data Cleaning with NLTK, NumPy\n",
    "        tokens = regexp_tokenize(data, pattern=r\"\\s|[\\/.,;'()]\", gaps=True)\n",
    "        words = []\n",
    "        for k in tokens:\n",
    "            if k not in en_stopwords and len(k) > 2:\n",
    "                k = lemma.lemmatize(k)\n",
    "            words.append(k)\n",
    "        data = \" \".join(words)\n",
    "\n",
    "        kill_words = [\n",
    "            \"iode\",\n",
    "            \"etwork\",\n",
    "            \"photo\",\n",
    "            \"optic\",\n",
    "        ]\n",
    "        flag = False\n",
    "        for kill_word in kill_words:\n",
    "            if kill_word in data:\n",
    "                # print(\"entered\")\n",
    "                df.loc[i, \"ZeroShot Judgement\"] = 0\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            continue\n",
    "\n",
    "        # Rienforcing the data with keyword\n",
    "        tokens = regexp_tokenize(data, pattern=r\"\\s|[\\&<>\\.,;'()]\", gaps=True)\n",
    "        for j in tokens:  # For every word in the data:\n",
    "            lc_j = j.lower()\n",
    "            if (\n",
    "                len(j) > 3\n",
    "            ):  # this excludes ie. the BJTs keyword, so be careful (but also filters out words like \"and\" etc)\n",
    "                # for k in regexp_tokenize(\"\".join(candidate_labels), pattern=r\"\\s|[\\&<>\\.,;'()]\", gaps=True):\n",
    "                for k in candidate_labels:\n",
    "                    lc_k = k.lower()\n",
    "                    if lc_j in lc_k:\n",
    "                        data += \" \" + j  # + \" \" + j)\n",
    "\n",
    "        # print(data)\n",
    "\n",
    "        # Some papers are excluded from the IEEE keywords.\n",
    "        input_text.append(data)\n",
    "        input_index.append(i)\n",
    "\n",
    "    # ~JUDGE~\n",
    "    print(\"#papers= %d\" % (len(input_text)))\n",
    "\n",
    "    start_judge = time.time()\n",
    "    index = 0\n",
    "    zero_shot_judgement = []\n",
    "    for final_judgement_dict in classifierGPU0(\n",
    "        input_text,\n",
    "        candidate_labels,\n",
    "        hypothesis_template=hypothesis_template,\n",
    "        batch_size=16,\n",
    "        multi_label=True,\n",
    "    ):\n",
    "        if (\n",
    "            final_judgement_dict[\"labels\"][0] in positive_outcome\n",
    "            and final_judgement_dict[\"scores\"][0] > perc_loop / 100.0\n",
    "        ):\n",
    "            zero_shot_judge = 1\n",
    "        else:\n",
    "            zero_shot_judge = 0\n",
    "        zero_shot_judgement.append(zero_shot_judge)\n",
    "    for c in range(len(input_index)):\n",
    "        df.loc[input_index[c], \"ZeroShot Judgement\"] = zero_shot_judgement[c]\n",
    "    print(\"#Papers = %d, Time elapsed: %.2f\" % (i + 1, (time.time() - start)))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Print differences\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, \"ZeroShot Judgement\"] != df.loc[i, \"Manual Judgement\"]:\n",
    "            count_diff += 1\n",
    "\n",
    "    percentages.append((count_all - count_diff) / count_all)\n",
    "    print(perc_loop / 100)\n",
    "    m, s = divmod(end - start, 60)\n",
    "    print(\"Time: %d min %0.3f sec\" % (m, s))\n",
    "    print(\n",
    "        \"From %d papers, the %d are CORRECT and the %d are WRONG ( %.2f percent correct )\\n\"\n",
    "        % (\n",
    "            count_all,\n",
    "            count_all - count_diff,\n",
    "            count_diff,\n",
    "            (100 * (count_all - count_diff) / count_all),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b2de2b-56dd-4f14-9785-498ec1b4302e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(0, 100, 5))\n",
    "for i in range(len(x)):\n",
    "    percentages[i] = percentages[i] * 100\n",
    "\n",
    "plt.scatter(x, percentages)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99891aad-759e-4e31-9bd2-7a0dd7b99f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP (python3.7)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
