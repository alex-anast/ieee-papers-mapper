{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8da2286-887b-4791-bfb7-4d55e3ffe0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET (reloading independent code fragment for now sampled excel)\n",
    "\n",
    "# Import CNN Zero-Shot & other necessary stuff\n",
    "from transformers import pipeline  # It takes time here\n",
    "\n",
    "# For data input and data cleaning\n",
    "import pandas as pd\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import NaN\n",
    "import time\n",
    "\n",
    "# Available directories (input yours for personal use)\n",
    "cep_data_dir = \"/home/aan0709@tmme/pcu-research-mapping/data/\"\n",
    "\n",
    "working_dir = \"TEST/\"  # CHANGE WORKING DIRECTORY INSIDE IEEE XPLORE FOLDER\n",
    "\n",
    "\n",
    "# For data importing\n",
    "def openExcel(excel_name, sheet_name, directory):\n",
    "    df = pd.read_excel(directory + excel_name + \".xlsx\", sheet_name=sheet_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleanIndexes(df):\n",
    "    temp_ls = df.columns\n",
    "    for i in df.columns:\n",
    "        if i == \"abstract\":\n",
    "            break\n",
    "        df.drop(columns=[i], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Select INPUT EXCEL\n",
    "file_name = \"TESTSET_Inverter_answered\"  # -> I have edited the Affiliations and the Keywords part\n",
    "\n",
    "df = openExcel(file_name, \"Sheet1\", cep_data_dir + working_dir)\n",
    "\n",
    "df = cleanIndexes(df)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a30330-7980-479c-b821-66f06c46403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds elapsed:  13.404227018356323\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "classifierGPU0 = pipeline(\n",
    "    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=2\n",
    ")\n",
    "end = time.time()\n",
    "print(\"seconds elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609df4e1-b162-40a9-9069-14598526c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 0\n",
    "\n",
    "positive_outcome = \"inverter\"\n",
    "\n",
    "candidate_labels = [positive_outcome]  # , negative_outcome]\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf52139-6f13-460d-b097-bf663d3a9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1\n",
    "\n",
    "positive_outcome = \"inverter\"  # dc/ac dc-ac motor vehicle\n",
    "negative_outcome = \"photovoltaic wireless low PV LV\"\n",
    "\n",
    "candidate_labels = [positive_outcome, negative_outcome]\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861d1a75-b2a2-452a-9db8-cbd62f43bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with MULTICLASS\n",
    "\n",
    "positive_outcome = [\"inverter\"]  # , \"dc/ac dc-ac\"]\n",
    "negative_outcome = [\"photovoltaic\", \"low voltage\"]\n",
    "\n",
    "candidate_labels = [positive_outcome, negative_outcome]\n",
    "temp = []\n",
    "temp = list(candidate_labels[0])\n",
    "candidate_labels[0] = list(candidate_labels[1])\n",
    "temp += candidate_labels[0]\n",
    "candidate_labels = temp\n",
    "\n",
    "positive_outcome = \";\".join(positive_outcome)\n",
    "negative_outcome = \";\".join(negative_outcome)\n",
    "\n",
    "hypothesis_template = \"The research is about {}?\"  # Categorization question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd87af6-9dac-48ae-95e8-86becf1fe10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 5.44\n",
      "0.0\n",
      "Time: 0 min 5.442 sec\n",
      "From 50 papers, the 38 are CORRECT and the 12 are WRONG ( 76.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 3.55\n",
      "0.05\n",
      "Time: 0 min 3.548 sec\n",
      "From 50 papers, the 40 are CORRECT and the 10 are WRONG ( 80.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 3.55\n",
      "0.1\n",
      "Time: 0 min 3.548 sec\n",
      "From 50 papers, the 41 are CORRECT and the 9 are WRONG ( 82.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 3.58\n",
      "0.15\n",
      "Time: 0 min 3.582 sec\n",
      "From 50 papers, the 41 are CORRECT and the 9 are WRONG ( 82.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 4.34\n",
      "0.2\n",
      "Time: 0 min 4.336 sec\n",
      "From 50 papers, the 42 are CORRECT and the 8 are WRONG ( 84.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 4.38\n",
      "0.25\n",
      "Time: 0 min 4.384 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 4.51\n",
      "0.3\n",
      "Time: 0 min 4.513 sec\n",
      "From 50 papers, the 45 are CORRECT and the 5 are WRONG ( 90.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 4.39\n",
      "0.35\n",
      "Time: 0 min 4.389 sec\n",
      "From 50 papers, the 43 are CORRECT and the 7 are WRONG ( 86.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 4.43\n",
      "0.4\n",
      "Time: 0 min 4.431 sec\n",
      "From 50 papers, the 43 are CORRECT and the 7 are WRONG ( 86.00 percent correct )\n",
      "\n",
      "#papers= 39\n",
      "#Papers = 50, Time elapsed: 4.47\n",
      "0.45\n",
      "Time: 0 min 4.468 sec\n",
      "From 50 papers, the 44 are CORRECT and the 6 are WRONG ( 88.00 percent correct )\n",
      "\n",
      "#papers= 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aan0709@tmme/.local/lib/python3.7/site-packages/transformers/pipelines/base.py:978: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Papers = 50, Time elapsed: 4.31\n",
      "0.5\n",
      "Time: 0 min 4.315 sec\n",
      "From 50 papers, the 42 are CORRECT and the 8 are WRONG ( 84.00 percent correct )\n",
      "\n",
      "#papers= 39\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "percentages = []\n",
    "for perc_loop in range(0, 100, 5):\n",
    "\n",
    "    en_stopwords = []\n",
    "    en_stopwords = list(get_stop_words(\"en\"))  # About 900 stopwords\n",
    "    nltk_words = list(stopwords.words(\"english\"))  # About 150 stopwords\n",
    "    en_stopwords.extend(nltk_words)\n",
    "\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    Author_Keywords = \"index_terms.author_terms.terms\"\n",
    "    IEEE_Keywords = \"index_terms.ieee_terms.terms\"\n",
    "\n",
    "    count_all = len(df)\n",
    "    count_diff = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    input_text = []  # This is the list that will be my dataset for the pipeline\n",
    "    input_index = []\n",
    "    final_judgement_dict = {}\n",
    "    for i in range(len(df)):\n",
    "        data = \"\"\n",
    "        # Where to search\n",
    "        if isinstance(df.loc[i, \"abstract\"], str):\n",
    "            data += df.loc[i, \"abstract\"]\n",
    "        if isinstance(df.loc[i, \"title\"], str):\n",
    "            data += \" \" + df.loc[i, \"title\"]\n",
    "        exception_to_str = df.loc[i, IEEE_Keywords]\n",
    "        if isinstance(exception_to_str, str):\n",
    "            exception_to_str = \" \".join(df.loc[i, IEEE_Keywords].split(\";\"))\n",
    "            data += \" \" + exception_to_str\n",
    "        exception_to_str = df.loc[i, Author_Keywords]\n",
    "        if isinstance(exception_to_str, str):\n",
    "            exception_to_str = \" \".join(df.loc[i, Author_Keywords].split(\";\"))\n",
    "            data += \" \" + exception_to_str\n",
    "            # printing_data = data\n",
    "\n",
    "        # Data Cleaning with NLTK, NumPy\n",
    "        tokens = regexp_tokenize(data, pattern=r\"\\s|[\\/.,;'()]\", gaps=True)\n",
    "        words = []\n",
    "        for k in tokens:\n",
    "            if k not in en_stopwords and len(k) > 2:\n",
    "                k = lemma.lemmatize(k)\n",
    "            words.append(k)\n",
    "        data = \" \".join(words)\n",
    "\n",
    "        kill_words = [\"photovoltaic\", \"wireless\", \"wire-less\"]\n",
    "        flag = False\n",
    "        for kill_word in kill_words:\n",
    "            if kill_word in data:\n",
    "                # print(\"entered\")\n",
    "                df.loc[i, \"ZeroShot Judgement\"] = 0\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            continue\n",
    "\n",
    "        # Rienforcing the data with keyword\n",
    "        tokens = regexp_tokenize(data, pattern=r\"\\s|[\\&<>\\.,;'()]\", gaps=True)\n",
    "        for j in tokens:  # For every word in the data:\n",
    "            lc_j = j.lower()\n",
    "            if (\n",
    "                len(j) > 3\n",
    "            ):  # this excludes ie. the BJTs keyword, so be careful (but also filters out words like \"and\" etc)\n",
    "                # for k in regexp_tokenize(\"\".join(candidate_labels), pattern=r\"\\s|[\\&<>\\.,;'()]\", gaps=True):\n",
    "                for k in candidate_labels:\n",
    "                    lc_k = k.lower()\n",
    "                    if lc_j in lc_k:\n",
    "                        data += \" \" + j  # + \" \" + j)\n",
    "\n",
    "        # print(data)\n",
    "\n",
    "        # Some papers are excluded from the IEEE keywords.\n",
    "        input_text.append(data)\n",
    "        input_index.append(i)\n",
    "\n",
    "    # ~JUDGE~\n",
    "    print(\"#papers= %d\" % (len(input_text)))\n",
    "\n",
    "    start_judge = time.time()\n",
    "    index = 0\n",
    "    zero_shot_judgement = []\n",
    "    for final_judgement_dict in classifierGPU0(\n",
    "        input_text,\n",
    "        candidate_labels,\n",
    "        hypothesis_template=hypothesis_template,\n",
    "        batch_size=8,\n",
    "        multi_label=True,\n",
    "    ):\n",
    "        if (\n",
    "            final_judgement_dict[\"labels\"][0] in positive_outcome\n",
    "            and final_judgement_dict[\"scores\"][0] > perc_loop / 100.0\n",
    "        ):\n",
    "            zero_shot_judge = 1\n",
    "        else:\n",
    "            zero_shot_judge = 0\n",
    "        zero_shot_judgement.append(zero_shot_judge)\n",
    "    for c in range(len(input_index)):\n",
    "        df.loc[input_index[c], \"ZeroShot Judgement\"] = zero_shot_judgement[c]\n",
    "    print(\"#Papers = %d, Time elapsed: %.2f\" % (i + 1, (time.time() - start)))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Print differences\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, \"ZeroShot Judgement\"] != df.loc[i, \"Manual Judgement\"]:\n",
    "            count_diff += 1\n",
    "\n",
    "    percentages.append((count_all - count_diff) / count_all)\n",
    "    print(perc_loop / 100)\n",
    "    m, s = divmod(end - start, 60)\n",
    "    print(\"Time: %d min %0.3f sec\" % (m, s))\n",
    "    print(\n",
    "        \"From %d papers, the %d are CORRECT and the %d are WRONG ( %.2f percent correct )\\n\"\n",
    "        % (\n",
    "            count_all,\n",
    "            count_all - count_diff,\n",
    "            count_diff,\n",
    "            (100 * (count_all - count_diff) / count_all),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6b23c-9ccf-4bb3-816e-2170997d0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(0, 100, 5))\n",
    "for i in range(len(x)):\n",
    "    percentages[i] = percentages[i] * 100\n",
    "\n",
    "plt.scatter(x, percentages)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feda4df-825a-4cb0-9853-8bf48d3fd6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP (python3.7)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
